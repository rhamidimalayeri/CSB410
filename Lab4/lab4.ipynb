{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349b3b9e",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo/blob/main/path-to-file)  \n",
    "**Students:** Replace `username`, `repo`, and `path-to-file` with your own GitHub username, repository name, and the path to this file.  \n",
    "After opening in Colab, go to **File → Save a copy to GitHub** (your repo) before editing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c70d39",
   "metadata": {},
   "source": [
    "# Lab 4 (Week 5) — Training Neural Networks (Student v2)\n",
    "\n",
    "This lab focuses on concepts demonstrated in your Week 5 scripts. Everything you need can be learned from these notebooks:\n",
    "\n",
    "- `5-quadratic_cost.ipynb`\n",
    "- `5-cross_entropy_cost.ipynb`\n",
    "- `5-intermediate_network_in_keras.ipynb`\n",
    "\n",
    "**Sections (aligned to scripts)**\n",
    "\n",
    "- D.1 Implement & compare cost functions (quadratic vs cross-entropy)\n",
    "- D.2 Saturation & gradients for sigmoid/tanh\n",
    "- D.5 Train a small Keras classifier (optimizer & batch-size experiments)\n",
    "- D.6 Intermediate network in Keras (dropout + callbacks)\n",
    "\n",
    "**Note:** D.3 and D.4 were removed so the lab only includes tasks supported directly by the scripts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44766c",
   "metadata": {},
   "source": [
    "## D.1 Implement & Compare Cost Functions (Quadratic vs Cross-Entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61744744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions only — write your own code. Use the Week 5 scripts:\n",
    "# - Reference: 5-quadratic_cost.ipynb, 5-cross_entropy_cost.ipynb\n",
    "#\n",
    "# Task:\n",
    "# 1) Implement two functions for a batch of predictions y_hat (probabilities) and binary labels y in {0,1}:\n",
    "#       quadratic_cost(y, y_hat)       -> mean of (y - y_hat)^2\n",
    "#       cross_entropy_cost(y, y_hat)   -> mean of -(y*log(y_hat+1e-9) + (1-y)*log(1-y_hat+1e-9))\n",
    "# 2) Create example arrays:\n",
    "#       y = [0,1,1,0,1]\n",
    "#       y_hat cases:\n",
    "#          a) poorly calibrated: [0.4,0.4,0.4,0.6,0.6]\n",
    "#          b) confident correct: [0.01,0.99,0.97,0.02,0.98]\n",
    "#          c) confident wrong:   [0.99,0.01,0.03,0.98,0.02]\n",
    "# 3) Print both costs for each case and briefly explain which loss penalizes confident-but-wrong predictions more.\n",
    "#\n",
    "# Hints:\n",
    "# - See your 5-quadratic_cost.ipynb for the MSE expression.\n",
    "# - See your 5-cross_entropy_cost.ipynb for the CE expression and why it penalizes confident wrong predictions.\n",
    "#\n",
    "# Expected output:\n",
    "# - Three lines like:\n",
    "#   poorly_calibrated  -> MSE: 0.280000  Cross-Entropy: 0.754105\n",
    "#   confident_correct  -> MSE: 0.000380  Cross-Entropy: 0.018193\n",
    "#   confident_wrong    -> MSE: 0.964380  Cross-Entropy: 4.108189 noticeably higher than MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc097c5",
   "metadata": {},
   "source": [
    "## D.2 Saturation & Gradients (Sigmoid/Tanh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d749eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions only — write your own code. Use the Week 5 scripts:\n",
    "# - Reference: 5-quadratic_cost.ipynb and 5-cross_entropy_cost.ipynb (activation/gradient intuition sections)\n",
    "#\n",
    "# Task:\n",
    "# 1) Implement sigmoid(x) and tanh(x) using NumPy.\n",
    "# 2) Implement derivatives:\n",
    "#       d_sigmoid(x) = sigmoid(x) * (1 - sigmoid(x))\n",
    "#       d_tanh(x)    = 1 - tanh(x)^2\n",
    "# 3) Create x = linspace(-10, 10, 400) and plot:\n",
    "#       - sigmoid(x) and tanh(x)\n",
    "#       - d_sigmoid(x) and d_tanh(x)\n",
    "# 4) Print min/max of derivatives to show how they saturate (approach 0) at large |x|.\n",
    "#\n",
    "# Hints:\n",
    "# - Revisit where the scripts discuss why cross-entropy helps with saturated sigmoid outputs.\n",
    "# - Use separate figures for clarity; do not customize colors.\n",
    "#\n",
    "# Expected output:\n",
    "# - Two figures (activations and derivatives) and printed min/max derivative values.\n",
    "# My printed values were\n",
    "# d_sigmoid min/max: 4.5395807735907655e-05 0.2499607455622195\n",
    "# d_tanh    min/max: 8.244614546626394e-09 0.9993721261856603\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656361e",
   "metadata": {},
   "source": [
    "## D.3 Keras Classifier: Optimizer & Batch-size Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38603616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions only — write your own code. Use the Week 5 Keras script:\n",
    "# - Reference: 5-intermediate_network_in_keras.ipynb\n",
    "#\n",
    "# Task:\n",
    "# 1) Build a small MLP classifier on MNIST:\n",
    "#       Input(784) -> Dense(128, 'relu') -> Dense(10, 'softmax')\n",
    "# 2) Try optimizer/batch-size combos:\n",
    "#       optimizers = ['sgd', 'adam']\n",
    "#       batch_sizes = [32, 128]\n",
    "# 3) For each combo:\n",
    "#       - Compile with loss='sparse_categorical_crossentropy', metrics=['accuracy']\n",
    "#       - Train 2–3 epochs with validation_split=0.1\n",
    "#       - Evaluate on the test set and print test accuracy\n",
    "# 4) Summarize which combo performed best and why (briefly).\n",
    "#\n",
    "# Hints:\n",
    "# - The example model in 5-intermediate_network_in_keras.ipynb shows the right layer shapes and compile settings.\n",
    "# - If local CPU is slow, run this in Colab with GPU enabled.\n",
    "#\n",
    "# Expected output:\n",
    "# - Four accuracy lines, one per (optimizer, batch_size) combination.\n",
    "# - Adam with moderate batch size generally performs well.\n",
    "# I got...\n",
    "# optimizer= sgd, batch_size= 32 -> test acc: 0.9151\n",
    "# optimizer= sgd, batch_size=128 -> test acc: 0.8804\n",
    "# optimizer=adam, batch_size= 32 -> test acc: 0.9666\n",
    "# optimizer=adam, batch_size=128 -> test acc: 0.9536\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6864844",
   "metadata": {},
   "source": [
    "## D.4 Intermediate Network in Keras (Dropout + Callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e3e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions only — write your own code. Use the Week 5 Keras script:\n",
    "# - Reference: 5-intermediate_network_in_keras.ipynb\n",
    "#\n",
    "# Task:\n",
    "# 1) Modify the model to:\n",
    "#       Input(784) -> Dense(256,'relu') -> Dropout(0.3) -> Dense(128,'relu') -> Dropout(0.3) -> Dense(10,'softmax')\n",
    "# 2) Add callbacks:\n",
    "#       EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)\n",
    "# 3) Train ~5 epochs (or until early stop), batch_size=128, validation_split=0.1\n",
    "# 4) Evaluate on test set; print test accuracy.\n",
    "#\n",
    "# Hints:\n",
    "# - Use the same preprocessing as in D.5.\n",
    "# - Compare your final accuracy to the simpler model from D.5.\n",
    "#\n",
    "# Expected output:\n",
    "# - I got test accuracy 0.9785\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161bd187",
   "metadata": {},
   "source": [
    "## Discussion (answer in complete sentences)\n",
    "\n",
    "1. Why does cross-entropy often converge faster than quadratic cost for classification with sigmoid/tanh outputs?\n",
    "2. In your optimizer/batch-size experiment, which combination worked best and why might that be?\n",
    "3. What effect did adding dropout and early stopping have on your deeper model compared to the simpler one?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab 4 (Week 5) — Student (v2)"
  },
  "kernelspec": {
   "display_name": "CSB410",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
