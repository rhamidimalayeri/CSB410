{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f43d8fe",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo/blob/main/path-to-file)  \n",
    "**Students:** Replace `username`, `repo`, and `path-to-file` with your own GitHub username, repository, and the path to **this** file.  \n",
    "After opening in Colab, go to **File → Save a copy to GitHub** and choose **your** repo before editing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe14e9",
   "metadata": {},
   "source": [
    "# Lab 3 (Week 4) — Artificial Neural Networks\n",
    "\n",
    "**Goal:** Practice forward propagation, softmax, and building a deeper network in Keras.  \n",
    "Commit your work to **your** GitHub repo via **File → Save a copy to GitHub**.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "- C.1 Inspect layers/weights of a simple model\n",
    "- C.2 Manual forward pass (ReLU hidden, Sigmoid output)\n",
    "- C.3 Softmax practice (numeric stability)\n",
    "- C.4 Deepen the network (compare to shallow)\n",
    "- C.5 Confusion matrix + misclassifications\n",
    "- C.6 Save & reload for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d87940",
   "metadata": {},
   "source": [
    "## C.1 Inspect Layers, Weights, and Shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions — write your own code below (no starter code).\n",
    "# Task:\n",
    "# 1) Build or load a shallow MNIST MLP in Keras:\n",
    "#       Input(784) -> Dense(128, 'relu') -> Dense(10, 'softmax')\n",
    "#    - Normalize MNIST to [0,1], flatten to 784.\n",
    "#    - Compile: Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'].\n",
    "#    - Train for 1 epoch (quick) so weights exist.\n",
    "# 2) Print each tensor's shape from model.get_weights().\n",
    "# 3) Briefly note: which shapes correspond to which layers (weights vs biases)?\n",
    "#\n",
    "# Expected output (example shapes for the architecture above):\n",
    "# - Tensor 0: (784, 128)   # Dense(128) weights\n",
    "# - Tensor 1: (128,)       # Dense(128) bias\n",
    "# - Tensor 2: (128, 10)    # Dense(10) weights\n",
    "# - Tensor 3: (10,)        # Dense(10) bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0b269",
   "metadata": {},
   "source": [
    "## C.2 Manual Forward Propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec19ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions — write your own code below.\n",
    "# Implement a manual forward pass with NumPy:\n",
    "# - X_ex: shape (1,3), choose any values (e.g., [[1.5, -1.2, 0.3]])\n",
    "# - W1:   shape (3,2), b1: shape (2,)  -> hidden layer with ReLU\n",
    "# - W2:   shape (2,1), b2: shape (1,)  -> output layer with Sigmoid\n",
    "# Steps:\n",
    "#   h = relu(X_ex @ W1 + b1)\n",
    "#   y = sigmoid(h @ W2 + b2)\n",
    "# Print the hidden vector and the final probability.\n",
    "#\n",
    "# Expected output:\n",
    "# - \"Hidden:\" printed array of shape (1, 2), non-negative values (due to ReLU).\n",
    "# - \"Output (sigmoid prob):\" a single value in (0, 1), e.g., [[0.62]] (exact value depends on your weights/biases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e35ef7",
   "metadata": {},
   "source": [
    "## C.3 Softmax Practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e6088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions — write your own code below.\n",
    "# Implement a numerically stable softmax(logits):\n",
    "#   shifted = logits - max(logits, axis=-1, keepdims=True)\n",
    "#   exps    = exp(shifted)\n",
    "#   probs   = exps / sum(exps, axis=-1, keepdims=True)\n",
    "# Test on three example rows of logits (varying lengths are okay) and print probabilities.\n",
    "# Also print the row-wise sums to verify they are ~1.0.\n",
    "#\n",
    "# Expected output:\n",
    "# - Lines like:\n",
    "#   Row 0 probs: [0.27 0.07 0.66] (sum≈1.000000)\n",
    "#   Row 1 probs: [0.39 0.35 0.26] (sum≈1.000000)\n",
    "#   Row 2 probs: [0.02 0.06 0.16 0.33 0.43] (sum≈1.000000)\n",
    "# - Exact numbers will differ, but each row's probabilities must sum to ~1.0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6b8ef",
   "metadata": {},
   "source": [
    "## C.4 Deepen the Network (Compare Capacity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions — write your own code below.\n",
    "# 1) Build a deeper MLP:\n",
    "#       Input(784) -> Dense(128, 'relu') -> Dense(64, 'relu') -> Dense(10, 'softmax')\n",
    "# 2) Train for ~5 epochs (batch_size=128, validation_split=0.1).\n",
    "# 3) Evaluate on test set; print test accuracy.\n",
    "# 4) Compare to the shallow model from C.1 — which performs better and why?\n",
    "#\n",
    "# Expected output:\n",
    "# - Training logs for ~5 epochs (loss/accuracy).\n",
    "# - One line printing test accuracy for the shallow model, number vary but should be around 0.94.\n",
    "# - One line printing test accuracy for the deep model, numbers vary but should be around 0.95\n",
    "# - A short text note (markdown cell or print) stating whether deep > shallow on your run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30346a61",
   "metadata": {},
   "source": [
    "## C.5 Confusion Matrix & Misclassifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions — write your own code below.\n",
    "# Using sklearn.metrics:\n",
    "#   - Compute confusion_matrix(y_test, y_pred)\n",
    "#   - Print classification_report(y_test, y_pred, digits=4)\n",
    "#   - Plot the confusion matrix (matplotlib, no custom colors needed)\n",
    "#   - Show up to 8 misclassified images with predicted vs true labels\n",
    "#\n",
    "# Expected output:\n",
    "# - Printed classification report with precision/recall/F1 per class and overall.\n",
    "# - Accuracy on this should be around 0.95.\n",
    "# - A confusion matrix plot (10x10 for MNIST) with counts per cell.\n",
    "# - A figure of up to 8 misclassified examples, each titled \"P:<pred> / T:<true>\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5600b917",
   "metadata": {},
   "source": [
    "## C.6 Save & Reload for Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions — write your own code below.\n",
    "# - Save your trained deep model (e.g., model.save('lab3_deep_model.keras'))\n",
    "# - Reload it with keras.models.load_model(...)\n",
    "# - Run a quick prediction on x_test[:5] and print the output shape to confirm\n",
    "#\n",
    "# Expected output:\n",
    "# - A confirmation message that saving/reloading succeeded.\n",
    "# - Printed example: \"Reloaded OK. Example prediction shape: (5, 10)\".\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab 3 (Week 4) — Student"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
