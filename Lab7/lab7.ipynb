{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cddc761",
   "metadata": {},
   "source": [
    "# Lab 7 — NLP & Sentiment Classification — Student (Revised)\n",
    "\n",
    "Complete each section using **only** techniques shown in these scripts:\n",
    "\n",
    "- `8-natural_language_preprocessing.ipynb`\n",
    "- `8-dense_sentiment_classifier.ipynb`\n",
    "- `8-rnn_sentiment_classifier.ipynb`\n",
    "- `8-lstm_sentiment_classifier.ipynb`\n",
    "\n",
    "For every step below, we tell you **where to find** the relevant code in the scripts and **what output to expect**.\n",
    "\n",
    "#### NOTE: You will need to use a GPU on Google Colab for this to run in a reasonable amount of time without memory running out. I suggest using A100 GPU. Edit -> Notebook Settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eadc43",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo/blob/main/path-to-file)  \n",
    "**Students:** Replace `username`, `repo`, and `path-to-file` with your own GitHub username, repository name, and the path to this file.  \n",
    "After opening in Colab, go to **File → Save a copy to GitHub** (your repo) before editing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218544b2",
   "metadata": {},
   "source": [
    "## G.1 Basic NLP Preprocessing\n",
    "\n",
    "**Reference script:** `8-natural_language_preprocessing.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721146c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (G.1): Implement the same preprocessing steps shown in the script.\n",
    "# Parse this string: \"This is a simple sentence. We are preprocessing it!\"\n",
    "# Follow the order: tokenize → lowercase → remove stopwords & punctuation → stem → bigrams.\n",
    "# Use the same NLTK utilities as in the script.\n",
    "\n",
    "# Your code here\n",
    "print(tokens, tokens_nostop, tokens_stem)\n",
    "print(\"Bigrams:\", bigrams[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d8fe8",
   "metadata": {},
   "source": [
    "**Expected output (G.1):**\n",
    "\n",
    "- Printed tokens showing punctuation (before cleaning): tokens\n",
    "- Printed tokens with stopwords/punctuation removed: tokens_nostop\n",
    "- Printed tokens after stemming (roots): tokens_stem\n",
    "- Printed bigrams (pairs of adjacent tokens): bigrams[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c90bd",
   "metadata": {},
   "source": [
    "## G.2 Dense Sentiment Classifier (IMDB)\n",
    "\n",
    "**Reference script:** `8-dense_sentiment_classifier.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd123202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (G.2): Recreate the dense IMDB pipeline as shown in the script.\n",
    "# 1) Define hyperparameters (n_unique_words, max_review_length, embedding_size, batch_size, epochs).\n",
    "# 2) Load IMDB (keras.datasets.imdb.load_data(num_words=n_unique_words)).\n",
    "# 3) Pad sequences (pad_sequences(..., maxlen=max_review_length)).\n",
    "# 4) Build Sequential model: Embedding → Flatten → Dense(250,'relu') → Dropout(0.2) → Dense(1,'sigmoid').\n",
    "# 5) Compile with loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'].\n",
    "# 6) Fit with validation_split=0.2; print model.summary() and final validation accuracy.\n",
    "# Follow md#29→code#30 and md#36→code#37 for architecture/compile details.\n",
    "\n",
    "# Your code here\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7228b",
   "metadata": {},
   "source": [
    "**Expected output (G.2):**\n",
    "\n",
    "- `model.summary()` shows layers in order: Embedding → Flatten → Dense(250) → Dropout(0.2) → Dense(1).\n",
    "- Training log prints `accuracy` and `val_accuracy` for each epoch (≥2 epochs).\n",
    "- A final printed metric like `{'dense_valid_accuracy': 0.xx}`. Typical values after 2 epochs are around 0.80 ± 0.05.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139b02a",
   "metadata": {},
   "source": [
    "## G.3 SimpleRNN Sentiment Classifier (IMDB)\n",
    "\n",
    "**Reference script:** `8-rnn_sentiment_classifier.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (G.3): Recreate the SimpleRNN IMDB pipeline exactly as in the script.\n",
    "# 1) Define hyperparameters consistent with the script (n_unique_words, max_review_length, embedding_size, n_rnn, dropout).\n",
    "# 2) Load and pad IMDB sequences.\n",
    "# 3) Build Sequential model: Embedding → SimpleRNN(units, dropout=...) → Dense(1,'sigmoid').\n",
    "# 4) Compile with loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'].\n",
    "# 5) Train with validation_split=0.2; print model.summary() and final validation accuracy.\n",
    "# Use the 'Design neural network architecture' and 'Configure model' sections as a guide.\n",
    "\n",
    "# Your code here\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7c2dd",
   "metadata": {},
   "source": [
    "**Expected output (G.3):**\n",
    "\n",
    "- `model.summary()` shows Embedding → SimpleRNN → Dense(1).\n",
    "- Training log with `accuracy` and `val_accuracy` each epoch.\n",
    "- Printed metric like `{'rnn_valid_accuracy': 0.xx}`. Expect lower than LSTM but higher than chance (e.g., ~0.80 ± 0.05 after a couple epochs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00b478",
   "metadata": {},
   "source": [
    "## G.4 LSTM Sentiment Classifier (IMDB)\n",
    "\n",
    "**Reference script:** `8-lstm_sentiment_classifier.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8abc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (G.4): Recreate the LSTM IMDB pipeline exactly as in the script.\n",
    "# 1) Match the hyperparameters in the LSTM script (n_unique_words, max_review_length, embedding_size, n_lstm, dropout).\n",
    "# 2) Load and pad IMDB sequences.\n",
    "# 3) Build Sequential model: Embedding → LSTM(units, dropout=...) → Dense(1,'sigmoid').\n",
    "# 4) Compile with loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'].\n",
    "# 5) Train with validation_split=0.2; print model.summary() and final validation accuracy.\n",
    "# Follow md#11→code#12 and md#14→code#15 for architecture & compile details.\n",
    "\n",
    "# Your code here\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ef40a",
   "metadata": {},
   "source": [
    "**Expected output (G.4):**\n",
    "\n",
    "- `model.summary()` shows Embedding → LSTM → Dense(1).\n",
    "- Training log with `accuracy` and `val_accuracy` for each epoch.\n",
    "- Printed metric like `{'lstm_valid_accuracy': 0.xx}`. Typically meets or exceeds SimpleRNN (e.g., ~0.84–0.88 after a couple epochs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5739999",
   "metadata": {},
   "source": [
    "## Discussion Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edebe61",
   "metadata": {},
   "source": [
    "1. In G.1, which preprocessing step produced the largest change in your tokens and why?\n",
    "2. Compare Dense vs RNN vs LSTM for sentiment tasks. When would you choose each and why?\n",
    "3. How do `n_unique_words` and `max_review_length` impact memory usage and accuracy?\n",
    "4. How does dropout affect training curves across G.2–G.4?\n",
    "5. If your LSTM underperforms the SimpleRNN, list three script-aligned changes to try first.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
