{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33adbd09",
   "metadata": {},
   "source": [
    "# Lab 2 — Artificial Neurons (Perceptron, Bias, Activations)\n",
    "\n",
    "This lab builds intuition for **artificial neurons**, focusing on:\n",
    "\n",
    "- A manual perceptron for the AND logic gate,\n",
    "- How **bias** shifts a decision boundary,\n",
    "- Implementing and plotting **activation functions**,\n",
    "- Comparing **activations** within a small Keras model,\n",
    "- A short **reflection** tying it together.\n",
    "\n",
    "**Workflow note:** Work directly in Colab with GPU off (CPU is fine for Week 3). Commit back to your own GitHub repo frequently via **File → Save a copy to GitHub**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70110a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo/blob/main/path-to-file)\n",
    "**Note:** Replace `username`, `repo`, and `path-to-file` above with your own GitHub username, repository name, and the exact path to this file in your repo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a71e02",
   "metadata": {},
   "source": [
    "## B.1 Manual Perceptron (NumPy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8656b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: implement a single perceptron to model the AND logic gate.\n",
    "# Instructions:\n",
    "# 1) Import NumPy.\n",
    "# 2) Implement a step(z) activation that returns 1 where z >= 0 and 0 otherwise.\n",
    "# 3) Create input data X with four rows: [0,0], [0,1], [1,0], [1,1]. dtype float32.\n",
    "# 4) Create target labels y = [0, 0, 0, 1] as int32.\n",
    "# 5) Choose weights w (length-2 NumPy array) and a bias scalar b for AND logic.\n",
    "#    Hint: equal weights (e.g., 1.0, 1.0) and a negative bias (e.g., -1.5) will require both inputs to be 1.\n",
    "# 6) Compute z = X @ w + b   (vectorized for all 4 samples).\n",
    "# 7) Compute y_hat = step(z).\n",
    "# 8) Print z, y_hat, and y to verify correctness.\n",
    "#\n",
    "# Expected output:\n",
    "# - z: an array of four values (weighted sums), with only the last one >= 0\n",
    "# - y_hat: [0 0 0 1]\n",
    "# - y:     [0 0 0 1]\n",
    "\n",
    "# Write your code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba81e62",
   "metadata": {},
   "source": [
    "## B.2 Bias Intuition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: observe how changing the bias shifts the decision boundary.\n",
    "# Instructions:\n",
    "# 1) Reuse your X, w from B.1 (or recreate them quickly).\n",
    "# 2) Create a list of bias values, e.g., [-2.0, -1.5, -1.0, -0.5, 0.0].\n",
    "# 3) For each bias in the list, compute y_hat = step(X @ w + b) and print the result.\n",
    "# 4) Briefly note which biases make the perceptron stricter or looser (in a markdown cell below).\n",
    "#\n",
    "# Expected output:\n",
    "# - Each line should show the bias value and predicted outputs (array of length 4).\n",
    "# - Example:\n",
    "#   b=-2.0: [0 0 0 0]\n",
    "#   b=-1.5: [0 0 0 1]\n",
    "#   b=-1.0: [0 0 1 1]\n",
    "#   ...\n",
    "# - This should demonstrate how lowering the bias makes it easier for the perceptron to activate.\n",
    "\n",
    "# Write your code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1decc",
   "metadata": {},
   "source": [
    "## B.3 Activation Functions: Implement & Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: implement three activation functions (without importing from ML frameworks) and plot them.\n",
    "# Instructions:\n",
    "# 1) Import NumPy and matplotlib.pyplot as plt.\n",
    "# 2) Implement functions:\n",
    "#       sigmoid(x) -> 1/(1+exp(-x))\n",
    "#       tanh(x)    -> use np.tanh(x)\n",
    "#       relu(x)    -> elementwise max(0, x)\n",
    "#    Each function should accept scalars or NumPy arrays and return a NumPy array.\n",
    "# 3) Create an array xs = np.linspace(-10, 10, 400).\n",
    "# 4) Make three separate plots (one for each activation). Do not customize colors.\n",
    "# 5) Add simple titles: \"Sigmoid\", \"Tanh\", \"ReLU\".\n",
    "#\n",
    "# Expected output:\n",
    "# - Three plots:\n",
    "#   1) Sigmoid: S-shaped curve from ~0 to ~1.\n",
    "#   2) Tanh: S-shaped curve from ~-1 to ~1, crossing at (0,0).\n",
    "#   3) ReLU: Flat at 0 for x<0, linearly increasing for x>0.\n",
    "\n",
    "# Write your code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67649d3",
   "metadata": {},
   "source": [
    "## B.4 Swap Activations in a Keras Model (MNIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: compare the impact of different hidden-layer activations on MNIST with a shallow network.\n",
    "# Instructions:\n",
    "# 1) Import TensorFlow/Keras:\n",
    "#       from tensorflow import keras\n",
    "#       from tensorflow.keras import layers\n",
    "# 2) Load MNIST using keras.datasets.mnist.load_data().\n",
    "# 3) Normalize pixel values to [0,1] and flatten images to vectors of length 784.\n",
    "# 4) Write a helper function train_with_activation(act: str) that:\n",
    "#       - Builds a Sequential model: Input(784) -> Dense(128, activation=act) -> Dense(10, activation='softmax')\n",
    "#       - Compiles with Adam(1e-3), sparse_categorical_crossentropy, metrics=['accuracy']\n",
    "#       - Fits for a small number of epochs (e.g., epochs=3, batch_size=128, validation_split=0.1)\n",
    "#       - Evaluates on the test set and returns the test accuracy (float).\n",
    "# 5) Loop over activations ['sigmoid', 'tanh', 'relu']:\n",
    "#       - Call the helper for each activation\n",
    "#       - Print the test accuracy for each\n",
    "# 6) In a short markdown cell below, summarize what you observe and provide a hypothesis why.\n",
    "#\n",
    "# Expected output:\n",
    "# - Three lines printed (one per activation), e.g.:\n",
    "#   sigmoid -> test acc: 0.92\n",
    "#   tanh    -> test acc: 0.94\n",
    "#   relu    -> test acc: 0.97\n",
    "# - ReLU is typically highest, but exact numbers will vary.\n",
    "\n",
    "# Write your code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d8f8a",
   "metadata": {},
   "source": [
    "## B.5 Reflection (Short Answer)\n",
    "\n",
    "- Which activation performed best in your experiment? Did that match your expectation?\n",
    "- How does the bias term affect the decision boundary in your perceptron example?\n",
    "- When might you prefer sigmoid, tanh, or ReLU in hidden layers? Explain briefly.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Week 3 Lab (Refactored — Directive Comments with Expected Output)"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
