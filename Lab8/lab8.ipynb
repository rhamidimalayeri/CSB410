{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f713caaf",
   "metadata": {},
   "source": [
    "# Lab 8 - Autoencoders\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo/blob/main/path-to-file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b3cb7",
   "metadata": {},
   "source": [
    "### How to find the right spots in the script\n",
    "\n",
    "- Open `17_autoencoders_gans_and_diffusion_models.ipynb` and use **Edit → Find** (or `Ctrl/Cmd+F`).\n",
    "- Search these exact phrases:\n",
    "  - **\"Autoencoder\"**, **\"Denoising\"**, **\"Sparse\"**, **\"VAE\"**, **\"KL\"**, **\"reparameterization\"**.\n",
    "- The cells you’ll need show Keras code with layers like `Dense`, `Conv2D`, `BatchNormalization`, and loss definitions. Mirror those patterns here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b3cd4",
   "metadata": {},
   "source": [
    "## H.1 Basic Autoencoder (Required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTIONS ONLY — WRITE YOUR OWN CODE BELOW.\n",
    "# Dataset & Preprocess:\n",
    "# - Use MNIST as in the script. In the script, search: \"Autoencoder\" and locate the first basic AE build cell.\n",
    "# - Mirror the preprocessing: scale images to [0,1]. For a Dense AE, reshape to (N, 784). For a Conv AE, keep (N, 28, 28, 1).\n",
    "#\n",
    "# Model (Dense example):\n",
    "# - In the script, look for a sequence with `Input(784) -> Dense(...) -> Dense(...)` followed by a symmetric decoder.\n",
    "# - Recreate: Input(784) -> Dense(128, 'relu') -> Dense(32, 'relu')  # encoder/code\n",
    "#             -> Dense(128, 'relu') -> Dense(784, 'sigmoid')         # decoder\n",
    "# - If you prefer Conv AE, in the script search: \"Conv2D\" near the autoencoder section and mirror that instead.\n",
    "#\n",
    "# Compile & Train:\n",
    "# - Match the script: optimizer='adam'. For Dense AE with sigmoid output, use loss='mse' or 'binary_crossentropy'.\n",
    "# - Train ~5 epochs with validation_split=0.1, batch_size=256. The script shows similar numbers.\n",
    "#\n",
    "# Evaluate & Save Reconstructions:\n",
    "# - Evaluate test reconstruction MSE as shown in the script (compute loss on x_test vs reconstruction).\n",
    "# - In the script, find the cell where they compute or print loss after training; follow that pattern.\n",
    "# - Save 10 reconstructed images for H.2 (keep originals and reconstructions).\n",
    "#\n",
    "# Expected output:\n",
    "# - Training/validation loss printed per epoch.\n",
    "# - Final test reconstruction MSE (typical quick run ~0.02–0.06).\n",
    "# I got Test MSE: 0.017725907266139984.\n",
    "# For MNIST autoencoders, anything in the 0.015–0.03 range is typical after a few training epochs \n",
    "# — so this shows your network is learning to reconstruct digits reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0b237",
   "metadata": {},
   "source": [
    "## H.2 Visualizing Reconstructions (Required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd75a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTIONS ONLY — WRITE YOUR OWN CODE BELOW.\n",
    "# Goal: Plot 10 original images vs their 10 reconstructions.\n",
    "# Where to look in the script:\n",
    "# - Search: \"reconstruction\" or \"imshow\" to find grid-plotting examples.\n",
    "# - You should see code using matplotlib with subplots to display images in a grid.\n",
    "# Steps:\n",
    "# - Select 10 test digits; call autoencoder to get reconstructions.\n",
    "# - Build a 2x10 grid: top row originals, bottom row reconstructions; use `cmap='gray'` for MNIST.\n",
    "# Expected output: A figure showing original (top) vs reconstruction (bottom) for 10 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddeb161",
   "metadata": {},
   "source": [
    "## H.3 Denoising Autoencoder (Required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6155df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTIONS ONLY — WRITE YOUR OWN CODE BELOW.\n",
    "# Where to look:\n",
    "# - In the script, search: \"Denoising\" and locate the section that adds Gaussian noise and trains on (x_noisy -> x_clean).\n",
    "# Noise & Training Pairs:\n",
    "# - Create x_noisy = clip(x + np.random.normal(0, sigma), 0, 1) using the sigma shown in the script (often ~0.3 for a strong effect).\n",
    "# - Train using (x_noisy as inputs, x_clean as targets), same optimizer/loss as basic AE unless the script uses a different one.\n",
    "# Evaluation & Viz:\n",
    "# - Compute MSE on the noisy test set mapped to clean targets; print it. Also compute the naive MSE between x_noisy and x_clean (no model) for comparison, as done in the script.\n",
    "# - Plot a small panel: noisy → denoised (model output) → clean, for at least 5 samples. In the script, search: \"denoise\" and \"imshow\".\n",
    "# Expected output:\n",
    "# - Printed denoising MSE that is clearly lower than naive noisy-vs-clean MSE.\n",
    "#  I got Naive MSE: 0.04661422318875196 Denoised MSE: 0.023852432146668434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a615b2",
   "metadata": {},
   "source": [
    "## H.4 Sparse Autoencoder (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1088615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTIONS ONLY — WRITE YOUR OWN CODE BELOW.\n",
    "# Where to look:\n",
    "# - In the script, search: \"Sparse\" or \"activity_regularizer\" to see examples of L1 penalties on the bottleneck.\n",
    "# What to do:\n",
    "# - Add `activity_regularizer=keras.regularizers.l1(1e-5)` (or the value shown in the script) to the code Dense layer.\n",
    "# - Retrain and compare reconstruction MSE to the basic AE, as the script suggests.\n",
    "# Expected output: Printed MSE comparison + 1–2 lines discussing the quality vs sparsity trade-off.\n",
    "# I got Sparse AE test MSE: 0.05206409841775894"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaf1ab",
   "metadata": {},
   "source": [
    "## H.5 Variational Autoencoder (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTIONS ONLY — WRITE YOUR OWN CODE BELOW.\n",
    "# Where to look:\n",
    "# - In the script, search: \"VAE\", \"KL\", and \"reparameterization\". You should find code that defines `mu`, `logvar`, and samples z.\n",
    "# What to implement:\n",
    "# - Encoder outputs mu and logvar; sample z = mu + exp(0.5*logvar) * eps.\n",
    "# - Decoder reconstructs x_hat. Total loss = recon_loss + KL term (often averaged over batch); follow the exact formula shown in the script.\n",
    "# - Train for ~10 epochs (or per script). Then sample random z to generate images; optionally visualize a 2D latent manifold if the code is provided.\n",
    "# Expected output: Printed training losses including KL; a grid of generated samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4778c37",
   "metadata": {},
   "source": [
    "## Discussion (answer in complete sentences)\n",
    "\n",
    "1. When might an autoencoder be preferable to PCA for dimensionality reduction, and why? (Hint: see slides section contrasting linear vs nonlinear methods.)\n",
    "\n",
    "2. Why does training with noisy inputs help a denoising autoencoder learn useful features? (Hint: refer to the denoising objective description in the script.)\n",
    "\n",
    "3. (Optional) What role does the KL divergence term play in a VAE, and how does it affect the learned latent space? (Hint: see the VAE loss derivation slide and the corresponding code cell in the script.)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab 8 — Autoencoders (Student, Referenced)"
  },
  "kernelspec": {
   "display_name": "CSB410",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
